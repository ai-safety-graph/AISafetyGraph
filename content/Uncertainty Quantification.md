# Uncertainty Quantification

## Summary
 Uncertainty Quantification is a critical aspect of machine learning and AI systems, particularly in real-world applications where input distributions may shift from the training data. This field encompasses various methods for estimating and evaluating the predictive uncertainty of models, including Bayesian and non-Bayesian approaches. Recent research has focused on developing algorithms that can assign probabilities to logical statements, provide robust uncertainty estimates for safety-critical applications, and handle dataset shifts. These techniques aim to improve model calibration, enhance decision-making in uncertain scenarios, and address challenges such as overconfidence in predictions on unseen data. Advancements in this area include the development of logical inductors, inductive coherence, and safe reinforcement learning frameworks that incorporate model uncertainty estimates. The ultimate goal of Uncertainty Quantification is to create AI systems that not only make accurate predictions but also reliably assess their own uncertainty, leading to more trustworthy and adaptable artificial intelligence.
## Research Papers

- [[Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift]]
- [[Asymptotic Logical Uncertainty and The Benford Test]]
- [[Safe Reinforcement Learning with Model Uncertainty Estimates]]
- [[Inductive Coherence]]
- [[Logical Induction]]
