# Few-Shot Learning

## Summary
 Few-shot learning is a critical area of research in AI that focuses on enabling models to learn and perform tasks with minimal training examples. Recent advancements in this field have primarily centered around large language models (LMs) and meta-learning techniques. While LMs have shown promising results in few-shot scenarios, studies suggest that their true few-shot abilities may be overestimated due to challenges in model selection and hyperparameter tuning with limited data. Meta-learning approaches, such as latent embedding optimization (LEO), have demonstrated state-of-the-art performance on few-shot classification tasks by learning data-dependent latent representations of model parameters. Additionally, instruction tuning has emerged as a simple yet effective method for improving zero-shot and few-shot learning abilities of large language models across various NLP tasks. These developments highlight the ongoing efforts to enhance AI systems' capacity to learn and adapt quickly with minimal examples, pushing the boundaries of few-shot learning capabilities.
## Research Papers

- [[True Few-Shot Learning with Language Models]]
- [[Meta-Learning with Latent Embedding Optimization]]
- [[Program Synthesis with Large Language Models]]
- [[Finetuned Language Models Are Zero-Shot Learners]]
