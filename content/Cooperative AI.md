# Cooperative AI

## Summary
 Cooperative AI is an emerging field of research that explores how artificial intelligence systems can be designed to collaborate effectively with each other and with humans. The subtopic encompasses various approaches to achieving mutually beneficial outcomes in multi-agent scenarios. One key area of focus is the development of bounded agents capable of self-reference and reasoning about each other's behaviors, as demonstrated by the parametric bounded Löb's theorem. This approach enables more robust cooperative equilibria in game-theoretic scenarios, such as the Prisoner's Dilemma, without relying on fragile conditions like program equality. Another important aspect of Cooperative AI involves multi-objective reinforcement learning (MORL) in situations where agents have differing beliefs and utility functions. Research in this area has revealed the need for policies that dynamically adjust priorities between agents' interests over time, taking into account each agent's beliefs and predictive accuracy. These advancements in Cooperative AI aim to create more sophisticated and adaptable systems for collaborative decision-making and problem-solving in complex, multi-agent environments.
## Research Papers

- [[Parametric Bounded Löb's Theorem and Robust Cooperation of Bounded Agents]]
- [[Toward negotiable reinforcement learning shifting priorities in Pareto optimal sequential decision-making]]
