# Human Understanding of Models

## Summary
 The subtopic of Human Understanding of Models explores how well people can comprehend and interact with machine learning models, particularly those designed to be interpretable. Research in this area investigates whether supposedly interpretable models actually achieve their intended effects, such as improving user trust, decision-making, and error detection. Experimental studies have shown that while certain model characteristics (like fewer features and transparent internals) can enhance a user's ability to simulate model predictions, they don't necessarily lead to better utilization of the model's outputs. Surprisingly, increased model transparency can sometimes hinder a user's ability to detect and correct model mistakes, possibly due to information overload. These findings highlight the complexity of human-model interactions and underscore the importance of empirical testing rather than relying solely on intuition when developing interpretable AI systems.
## Research Papers

- [[Manipulating and Measuring Model Interpretability]]
