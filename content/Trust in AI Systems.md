# Trust in AI Systems

## Summary
 Trust in AI systems is a crucial aspect of human-AI interaction, particularly when it comes to deploying and supervising AI-powered robots or agents. A key challenge in establishing appropriate trust is the difficulty humans face in understanding the capabilities and decision-making processes of complex neural network policies. To address this issue, researchers have proposed focusing on "critical states" â€“ key moments in a task where specific actions are particularly important. By showcasing these critical states to users, AI systems can help humans build more accurate mental models of their capabilities and limitations. This approach enables users to make more informed decisions about when to deploy AI systems and when to intervene during task execution. Ultimately, this method of transparency and communication about critical states can lead to more appropriate levels of trust in AI systems, improving the effectiveness and safety of human-AI collaboration.
## Research Papers

- [[Establishing Appropriate Trust via Critical States]]
