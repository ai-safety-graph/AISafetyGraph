# Scaling Laws

## Summary
 Scaling laws in AI research describe how model performance improves as a function of key factors such as model size, dataset size, and computational resources. These relationships often follow power-law patterns that can span several orders of magnitude. Studies have shown that larger models are more sample-efficient, and that optimal training involves using very large models on relatively modest datasets. The scaling behavior extends to transfer learning, where pre-training effectively multiplies the fine-tuning dataset size. Researchers have also proposed theoretical explanations for these scaling laws, suggesting that they arise from neural networks performing regression on data manifolds of intrinsic dimension d, with scaling exponents related to this dimension. Understanding these scaling laws helps researchers optimize resource allocation and predict performance improvements as models and datasets grow.
## Research Papers

- [[Scaling Laws for Neural Language Models]]
- [[Scaling Laws for Transfer]]
- [[A Neural Scaling Law from the Dimension of the Data Manifold]]
