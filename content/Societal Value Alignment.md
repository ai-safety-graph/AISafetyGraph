# Societal Value Alignment

## Summary
 Societal Value Alignment is a critical challenge in AI development, addressing the potential conflicts between individual and societal benefits arising from AI adoption. This subtopic explores how AI systems can be designed and regulated to align with human values and existing social conventions. Research in this area employs various approaches, including game-theoretical models and multi-agent reinforcement learning, to simulate and analyze the effects of different AI norms and policies on society. Studies have shown that without proper regulation, selfish AI systems may gain advantages over more utilitarian alternatives, potentially leading to increased inequality. However, it is possible to develop AI systems that follow human-conscious policies, resulting in equilibria that benefit both adopters and non-adopters while increasing overall societal wealth. Additionally, techniques such as observationally augmented self-play have been proposed to help AI agents learn and adapt to existing social conventions, improving their ability to coordinate effectively with humans in various domains like traffic navigation, communication, and team coordination.
## Research Papers

- [[Norms for Beneficial A.I. A Computational Analysis of the Societal Value Alignment Problem]]
- [[Learning Existing Social Conventions via Observationally Augmented Self-Play]]
